{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Distribution\n",
    "\n",
    "[Back to index](https://shotahorii.github.io/math-for-ds/)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "1. Definition\n",
    "2. Proof that Multinomial distribution is normalised\n",
    "3. Expected value\n",
    "4. Variance\n",
    "5. Likelihood function\n",
    "6. Maximum likelihood estimator\n",
    "7. Conjugate prior\n",
    "8. Posterior distribution\n",
    "9. Predictive distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "Suppose there's a deck of $K$ cards (each card has different number $1,2,3,..., K$), and you draw a card from the deck $N$ times, with replacing the extracted card after each draw. If you represent the number of times you extract $k$-th card after $N$ draws as $m_k$, the result of this experiment is described by below discrete variable. \n",
    "\n",
    "${\\bf m} = (m_1,m_2,...,m_K)^T$\n",
    "\n",
    "$where \\,\\,\\, m_k \\in \\mathbb{N}_0, \\,\\,\\, \\sum^K_{k=1}m_k=N$\n",
    "\n",
    "For example, there're 5 cards in the deck ($K=5$), and you draw 6 times from the deck ($N=6$) with the following result: 3 times $1$-st card, 1 time $2$-nd card, and 2 times $3$-rd card. Then ${\\bf m}$ is described as below.\n",
    "\n",
    "${\\bf m} = (3,1,2,0,0)^T$\n",
    "\n",
    "Probability of each ${\\bf m}$ occurs is described by Multinomial distribution, and defined as below. \n",
    "\n",
    "$ Mult({\\bf m}|{\\bf \\mu},N)=\\frac{N!}{m_1!m_2!...m_K!}\\prod^K_{k=1} \\mu_k^{m_k}$\n",
    "\n",
    "$where \\,\\,\\, {\\bf \\mu} = (\\mu_1,\\mu_2,...,\\mu_K)^T, \\,\\,\\, \\mu_k \\in [0,1], \\,\\,\\, \\sum^K_{k=1}\\mu_k=1$ \n",
    "\n",
    "**Example**\n",
    "\n",
    "For example, for ${\\bf m}_{example} = (2,1,0,0,0)^T$ is below.\n",
    "\n",
    "$Mult({\\bf m}_{example}|{\\bf \\mu})=\\frac{3!}{2!1!0!0!0!}\\mu_1^2\\mu_2^1\\mu_3^0\\mu_4^0\\mu_5^0 = 3\\mu_1^2\\mu_2$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Proof that Multinomial distribution is normalised\n",
    "\n",
    "By [multinomial theorem](https://github.com/shotahorii/math-for-ds/blob/master/content/others/binomial_and_multinomial_theorem.ipynb), below is true.\n",
    "\n",
    "$(\\mu_1+\\mu_2+...+\\mu_K)^N = \\sum_{m_1+m_2+...+m_K=N} \\frac{N!}{m_1!m_2!...m_K!} \\prod_{k=1}^K \\mu_k^{m_k}$\n",
    "\n",
    "$= \\sum_{m_1+m_2+...+m_K=N} Mult({\\bf m}|{\\bf \\mu},N)$\n",
    "\n",
    "Note: $\\sum^K_{k=1}\\mu_k=1$\n",
    "\n",
    "$1^N = 1 = \\sum_{m_1+m_2+...+m_K=N} Mult({\\bf m}|{\\bf \\mu},N)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Expected value\n",
    "\n",
    "$E[{\\bf x}|{\\bf \\mu}] = \\sum^K_{k=1}{\\bf x}_{(k)}Cat({\\bf x}_{(k)}|{\\bf \\mu})$ \n",
    "\n",
    "$= \\sum^K_{k=1}\\mu_k{\\bf x}_{(k)}$ \n",
    "\n",
    "$=(\\mu_1,0,...,0)^T+(0,\\mu_2,...,0)^T+...+(0,0,...,\\mu_K)^T$\n",
    "\n",
    "$=(\\mu_1,\\mu_2,...,\\mu_K)^T = {\\bf \\mu}$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Variance\n",
    "Considering ${\\bf x} = (x_1,x_2,...,x_K)^T$ as a $K$ dimentional random vector, its $k$-th random variable ($x_k$) 's variance is the $k$-th diagonal element of the covariance matrix of ${\\bf x}$. \n",
    "\n",
    "$V[x_k] = E[(x_k-E[x_k])(x_k-E[x_k])] = E[x_k^2]-(E[x_k])^2$\n",
    "\n",
    "$= 1 \\cdot \\mu_k + 0 \\cdot (1-\\mu_k) - \\mu_k^2$\n",
    "\n",
    "$= \\mu_k(1-\\mu_k)$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Likelihood function\n",
    "Assume we have a data set $D = \\{{\\bf x}_1, {\\bf x}_2, ..., {\\bf x}_N\\}$. Assuming that each of ${\\bf x}_n$ is indepentently obtained from $Cat({\\bf x}|{\\bf \\mu})$, likelihood function is obtained as below.\n",
    "\n",
    "$P(D|{\\bf \\mu}) = \\prod^N_{n=1} Cat({\\bf x}_n|{\\bf \\mu}) = \\prod^N_{n=1} \\prod^K_{k=1} \\mu_k^{x_k^{(n)}}$\n",
    "\n",
    "$= \\prod^K_{k=1} \\mu_k^{(\\sum_{n=1}^Nx_k^{(n)})}$\n",
    "\n",
    "Let: $m_k = \\sum_{n=1}^Nx_k^{(n)}$ ,which is the number of data points with $x_k=1$\n",
    "\n",
    "$= \\prod^K_{k=1} \\mu_k^{m_k}$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Maximum likelihood estimator\n",
    "Maximise $P(D|{\\bf \\mu})$ under the constraint $\\sum^K_{k=1}\\mu_k=1$. For the computational simplicity, maximise log likelihood.\n",
    "\n",
    "$lnP(D|{\\bf \\mu}) = ln\\prod^K_{k=1} \\mu_k^{m_k} = \\sum_{k=1}^K m_k ln \\mu_k$\n",
    "\n",
    "With the method of Lagrange multiplier, \n",
    "\n",
    "$L({\\bf \\mu},\\lambda) = \\sum_{k=1}^K m_k ln \\mu_k - \\lambda (\\sum^K_{k=1}\\mu_k-1)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\mu_1} = \\frac{\\partial L}{\\partial \\mu_2} = ... = \\frac{\\partial L}{\\partial \\mu_K} =\\frac{\\partial L}{\\partial \\lambda} = 0 $\n",
    "\n",
    "Let's think about the partial derivative of L with respect to $\\mu_k$ and set to zero.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\mu_k} = \\frac{m_k}{\\mu_k} - \\lambda = 0$\n",
    "\n",
    "Hence, $\\mu_k = \\frac{m_k}{\\lambda}$\n",
    "\n",
    "Then, the partial derivative of L with respect to $\\lambda$ and set to zero.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\lambda} = -(\\sum^K_{k=1}\\mu_k-1) = 0 $\n",
    "\n",
    "$\\sum^K_{k=1}\\mu_k = 1$\n",
    "\n",
    "$\\sum^K_{k=1}\\frac{m_k}{\\lambda} = 1$\n",
    "\n",
    "$\\lambda = \\sum^K_{k=1}m_k$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$\\mu_k = \\frac{m_k}{\\sum^K_{k=1}m_k} = \\frac{m_k}{N}$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conjugate prior\n",
    "Dirichlet distribution\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 8. Posterior distribution\n",
    "Having a set of observed x, $D = \\{x_1, x_2, ..., x_N\\}$, posterior distribution of $\\mu$ is below.\n",
    "\n",
    "$P({\\bf \\mu}|D) = \\frac{P(D|{\\bf \\mu})P({\\bf \\mu})}{P(D)} = \\frac{\\{\\prod^N_{n=1}P({\\bf x}_n|{\\bf \\mu})\\}P({\\bf \\mu})}{P(D)}$\n",
    "\n",
    "\n",
    "For computational simplicity, calculate log.\n",
    "\n",
    "$lnP({\\bf \\mu}|D) = ln\\prod^N_{n=1}P({\\bf x}_n|{\\bf \\mu}) + lnP({\\bf \\mu}) - lnP(D)$\n",
    "\n",
    "Now, as we are interested in the posterior distribution of $\\mu$, consider non-$\\mu$ part as $const$.\n",
    "\n",
    "$= ln\\prod^N_{n=1}P({\\bf x}_n|{\\bf \\mu}) + lnP({\\bf \\mu}) + const$\n",
    "\n",
    "$= ln\\prod^N_{n=1}Cat({\\bf x}_n|{\\bf \\mu}) + lnDir({\\bf \\mu}|{\\bf \\alpha}) + const$\n",
    "\n",
    "$= ln\\prod^N_{n=1}\\prod^K_{k=1}\\mu_k^{x_k^{(n)}} + ln \\frac{1}{B({\\bf \\alpha})} \\prod^K_{k=1} \\mu_k^{\\alpha_k-1} + const$\n",
    "\n",
    "Let: $m_k = \\sum_{n=1}^Nx_k^{(n)}$\n",
    "\n",
    "$=ln\\prod^K_{k=1} \\mu_k^{m_k} + ln\\prod^K_{k=1} \\mu_k^{\\alpha_k-1} + const$\n",
    "\n",
    "$=ln\\prod^K_{k=1} \\mu_k^{m_k+\\alpha_k-1} + const$\n",
    "\n",
    "This is the form of (log) Dirichlet distribution with the const as its normalisation constant. Hence we can tell below.\n",
    "\n",
    "$P({\\bf \\mu}|D) = Dir({\\bf \\mu}|{\\bf \\hat{\\alpha}})$\n",
    "\n",
    "$where \\,\\,\\, {\\bf \\hat{\\alpha}} = (\\hat{\\alpha}_1,\\hat{\\alpha}_2,...,\\hat{\\alpha}_K)^T,\\,\\,\\,\n",
    "\\hat{\\alpha}_k = \\alpha_k + m_k$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 9. Predictive distribution\n",
    "The predictive distribution of ${\\bf x}_* = (x_1^*,x_2^*,...,x_K^*)^T$, value generated from Categorical distribution with parameter ${\\bf \\mu}$, is calculated by using likelihood and prior distribution as below.\n",
    "\n",
    "$P({\\bf x}_*) = \\int P({\\bf x}_*|{\\bf \\mu})P({\\bf \\mu})d{\\bf \\mu}$\n",
    "\n",
    "$= \\int Cat({\\bf x}_*|{\\bf \\mu})Dir({\\bf \\mu}|{\\bf \\alpha})d{\\bf \\mu}$\n",
    "\n",
    "$= \\int \\prod^K_{k=1} \\mu_k^{x_k^{(*)}}  \\frac{1}{B({\\bf \\alpha})}\\prod^K_{k=1} \\mu_k^{\\alpha_k-1}d{\\bf \\mu}$\n",
    "\n",
    "$= \\frac{1}{B({\\bf \\alpha})}\\int \\prod^K_{k=1} \\mu_k^{x_k^{(*)}+\\alpha_k-1}d{\\bf \\mu}$\n",
    "\n",
    "Note: $\\int Dir({\\bf \\mu}|{\\bf A})d{\\bf \\mu}=1 \\Longleftrightarrow\n",
    "\\int \\frac{1}{B({\\bf A})} \\prod^K_{k=1} \\mu_k^{A_k-1}d{\\bf \\mu}=1 \\Longleftrightarrow\n",
    "\\int \\prod^K_{k=1} \\mu_k^{A_k-1}d{\\bf \\mu}=B({\\bf A})$\n",
    "\n",
    "$= \\frac{1}{B({\\bf \\alpha})}B({\\bf \\alpha}+{\\bf x}_*)$\n",
    "\n",
    "$= \\frac{\\Gamma(\\sum_{k=1}^K\\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}\n",
    "\\frac{\\prod_{k=1}^K\\Gamma(\\alpha_k+x_k^{(*)})}{\\Gamma(\\sum_{k=1}^K\\alpha_k+x_k^{(*)})}$\n",
    "\n",
    "Note: $\\sum_{k=1}^K x_k^{(*)} = 1$\n",
    "\n",
    "$= \\frac{\\Gamma(\\sum_{k=1}^K\\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}\n",
    "\\frac{\\prod_{k=1}^K\\Gamma(\\alpha_k+x_k^{(*)})}{\\Gamma(1+\\sum_{k=1}^K\\alpha_k)}$\n",
    "\n",
    "$= \\frac{1}{\\sum_{k=1}^K\\alpha_k}\\frac{\\prod_{k=1}^K\\Gamma(\\alpha_k+x_k^{(*)})}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}$\n",
    "\n",
    "Let's think about the case where $k'$-th element is $1$. For example, if $k'=2$ then ${\\bf x}_*=(0,1,0,...,0)^T$.\n",
    "\n",
    "$P(x_{k'}^{(*)} =1) = \\frac{1}{\\sum_{k=1}^K\\alpha_k}\\frac{\\Gamma(\\alpha_{k'}+1)}{\\Gamma(\\alpha_{k'})} \n",
    "=\\frac{\\alpha_{k'}}{\\sum_{k=1}^K\\alpha_k}$\n",
    "\n",
    "Generalising this,\n",
    "\n",
    "$P({\\bf x_*}) = \\prod_{k=1}^K (\\frac{\\alpha_k}{\\sum_{i=1}^K\\alpha_i})^{x_k^{(*)}}$\n",
    "\n",
    "$=Cat({\\bf x_*}|{\\bf \\hat{\\mu}})$\n",
    "\n",
    "$where \\,\\,\\, {\\bf \\hat{\\mu}} = (\\hat{\\mu}_1,\\hat{\\mu}_2,...,\\hat{\\mu}_K)^T,\\,\\,\\,\n",
    "\\hat{\\mu}_k = \\frac{\\alpha_k}{\\sum_{i=1}^K\\alpha_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
