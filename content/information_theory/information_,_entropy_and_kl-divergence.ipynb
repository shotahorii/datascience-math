{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information, Entropy and KL-divergence\n",
    "\n",
    "[Back to index](https://shotahorii.github.io/math-for-ds/)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "1. **Introduction**\n",
    "2. **Self-Information**  \n",
    "2.1. Definition  \n",
    "2.2. Example  \n",
    "3. **Entropy**  \n",
    "3.1. Definition  \n",
    "3.2. Example  \n",
    "4. **Cross Entropy**  \n",
    "4.1. Definition  \n",
    "4.3. Example  \n",
    "5. **KL-divergence**  \n",
    "5.1. Definition  \n",
    "5.2. Example  \n",
    "6. **Mutual Information**  \n",
    "6.1. Definition  \n",
    "6.2. Example\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "The minimum amount of average bit-size (or nat-size) to convey information of an observed event $ X=x $ under a probability distribution $P(X)$ is depending only on the probability distribution $P(X)$.  \n",
    "And the minimum average bit-size (or nat-size) is achived when assigning $-logP(\\omega)$ length code to the event occuring in probability of $P(\\omega)$. The unit of length is **bit** when the base of $log$ is $2$ and **nat** when the base of $log$ is $e$.  \n",
    "The bit-size (or nat-size) used to convey an event $X=\\omega$ with this encoding is called **Self-information** of the event. And the average amount of the bit-size (or nat-size) over all possible realisation of $X$ in $P(X)$ is called **Entropy** of the probability distribution $P(X)$.  \n",
    "\n",
    "I'm going to use below example throughout this notebook.  \n",
    "> Assume that we observe weather of 3 different countries (A,B and C) every morning, and convey the information to the meteorological bureau.  \n",
    "We know the probability distributions of those countries' weather as below.  \n",
    "**(Country A) Sunny=1/4, Cloudy=1/4, Rain=1/4, Snow=1/4**  \n",
    "**(Country B) Sunny=1/2, Cloudy=1/4, Rain=1/8, Snow=1/8**  \n",
    "**(Country C) Sunny=1, Cloudy=0, Rain=0, Snow=0**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Self-Information\n",
    "### 2.1. Definition\n",
    "Given a random variable $X$ with probability mass function $P_X(x)$, the self-information of measuring $X=x$ is\n",
    "\n",
    "$I_X(x) = -logP_X(x)$ \n",
    "\n",
    "### 2.2. Example\n",
    "When the actual weather is observed in the 3 countries, necessary bit-size to convey the information is as below.\n",
    "#### Country A\n",
    "$I_A(Sunny) = -log_2P_A(Sunny) = -log_2\\frac{1}{4} = 2$  \n",
    "$I_A(Cloudy) = -log_2P_A(Cloudy) = -log_2\\frac{1}{4} = 2$  \n",
    "$I_A(Rain) = -log_2P_A(Rain) = -log_2\\frac{1}{4} = 2$  \n",
    "$I_A(Snow) = -log_2P_A(Snow) = -log_2\\frac{1}{4} = 2$\n",
    "\n",
    "#### Country B\n",
    "$I_B(Sunny) = -log_2P_B(Sunny) = -log_2\\frac{1}{2} = 1$  \n",
    "$I_B(Cloudy) = -log_2P_B(Cloudy) = -log_2\\frac{1}{4} = 2$  \n",
    "$I_B(Rain) = -log_2P_B(Rain) = -log_2\\frac{1}{8} = 3$  \n",
    "$I_B(Snow) = -log_2P_B(Snow) = -log_2\\frac{1}{8} = 3$\n",
    "\n",
    "#### Country C\n",
    "$I_C(Sunny) = I_C(Cloudy) = I_C(Rain) = I_C(Snow) =0$ (As this country is always Sunny, no need to send any information.)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Entropy\n",
    "### 3.1. Definition\n",
    "\n",
    "$H[P_X] = - \\sum P_X(x) logP_X(x) = E_{P_X}[-logP_X(x)] = \\sum P_X(x) I_X(x)$ \n",
    "\n",
    "### 3.2. Example\n",
    "Average bit-size to convey the weather information in each country is as below. \n",
    "#### Country A\n",
    "$H[P_A] = P_A(Sunny)I_A(Sunny) + P_A(Cloudy)I_A(Cloudy) + P_A(Rain)I_A(Rain) + P_A(Snow)I_A(Snow) = \n",
    "\\frac{1}{4}\\cdot2 + \\frac{1}{4}\\cdot2 + \\frac{1}{4}\\cdot2 + \\frac{1}{4}\\cdot2 = 2$\n",
    "\n",
    "**Encoding**: $Sunny=00, Cloudy=01, Rain=10, Snow=11$\n",
    "\n",
    "#### Country B\n",
    "$H[P_B] = P_B(Sunny)I_B(Sunny) + P_B(Cloudy)I_B(Cloudy) + P_B(Rain)I_B(Rain) + P_B(Snow)I_B(Snow) = \n",
    "\\frac{1}{2}\\cdot1 + \\frac{1}{4}\\cdot2 + \\frac{1}{8}\\cdot3 + \\frac{1}{8}\\cdot3 = 1.75$\n",
    "\n",
    "**Encoding**: $Sunny=0, Cloudy=10, Rain=110, Snow=111$\n",
    "\n",
    "#### Country C\n",
    "$H[P_C] = 0$\n",
    "\n",
    "**Encoding**: N/A\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Cross Entropy\n",
    "### 4.1. Definition\n",
    "\n",
    "$H(p,q) = E_p[-logq] = -\\sum p(x)logq(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
